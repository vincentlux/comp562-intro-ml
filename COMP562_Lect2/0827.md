# Review of probability

## Bayes Rule

<img src="https://latex.codecogs.com/gif.latex?p(B|A)&space;=&space;\frac{p(A,B)}{p(A)}&space;=&space;\frac{p(A|B)p(B)}{p(A)}&space;=&space;\frac{p(A|B)p(B)}{\sum_b&space;p(A|B=b)p(B=b)}" title="p(B|A) = \frac{p(A,B)}{p(A)} = \frac{p(A|B)p(B)}{p(A)} = \frac{p(A|B)p(B)}{\sum_b p(A|B=b)p(B=b)}" />

## Gaussian

<img src="https://latex.codecogs.com/gif.latex?$$&space;\begin{aligned}&space;X&space;&\sim&space;\mathcal{N}(\mu,\sigma^2)&space;\\&space;\\&space;p(X&space;=&space;x|\mu,\sigma^2)&=&space;\frac{1}{\sqrt{2\pi\sigma^2}}&space;e^{-\frac{1}{2\sigma^2}(x-\mu)^2}&space;\end{aligned}&space;$$" title="$$ \begin{aligned} X &\sim \mathcal{N}(\mu,\sigma^2) \\ \\ p(X = x|\mu,\sigma^2)&= \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2} \end{aligned} $$" />

### Why Gaussian important?
* only two params; easy to interpret


## Parameters and Likelihood Function

Each of the coin tosses can be thought of as a realization of a random variable

Hence we can write probability of the data X (from Bernoulli)

<img src="https://latex.codecogs.com/gif.latex?$$&space;p(\mathbf{x}|\theta)&space;=&space;\prod_{i=1}^N&space;p(X&space;=&space;x_i|\theta)&space;=&space;\prod_{i=1}^N&space;\theta^{x_i}(1-\theta)^{1-x_i}&space;$$" title="$$ p(\mathbf{x}|\theta) = \prod_{i=1}^N p(X = x_i|\theta) = \prod_{i=1}^N \theta^{x_i}(1-\theta)^{1-x_i} $$" />


## Parameters and Likelihood Function

Hence, we can try to find parameter $\theta$ for which $p(\mathbf{x}|\theta)$ is the largest

$p(\mathbf{x}|\theta)$ can be seen as a function of **parameter** $\theta$

This function is called *likelihood*

<img src="https://latex.codecogs.com/gif.latex?$$&space;\mathcal{L(\theta|\mathbf{x})}&space;=&space;p(\mathbf{x}|\theta)&space;$$" title="$$ \mathcal{L(\theta|\mathbf{x})} = p(\mathbf{x}|\theta) $$" />

$\theta$ which results in the largest likelihood is called **maximum-likelihood estimate**

In many cases, learning is nothing more than maximizing likelihood


## Log likelihood

<img src="https://latex.codecogs.com/gif.latex?$$&space;\log&space;\mathcal{L}(\theta|\mathbf{x})&space;=&space;\log&space;p(\mathbf{x}|\theta)&space;=&space;\log&space;\prod_i&space;p(x_i|\theta)&space;=&space;\sum_i&space;\log&space;p(x_i|\theta)&space;$$" title="$$ \log \mathcal{L}(\theta|\mathbf{x}) = \log p(\mathbf{x}|\theta) = \log \prod_i p(x_i|\theta) = \sum_i \log p(x_i|\theta) $$" />

## Convex optimization
?










